{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9bca76b130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = \"MinitaurBulletEnv-v0\"\n",
    "PROC_COUNT = 1\n",
    "NUM_ENVS_PER_PROC = 1\n",
    "BATCH_SIZE = 128\n",
    "LR_AGENT = 1e-4\n",
    "LR_CRITIC = 1e-4\n",
    "BUFFER_MAXLEN = 100_000\n",
    "REPLAY_INITIAL = 10_000\n",
    "REPLAY_START_SIZE = 2_000\n",
    "DEV = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEV)\n",
    "REPEAT_STEPS = 5\n",
    "MAX_EPSILON = 1.0\n",
    "MIN_EPSILON = 1e-3\n",
    "EPSILON_DECAY_LAST_ITR = 1e5\n",
    "GAMMA = 0.99\n",
    "MODEL_SAVE_PATH = \"models/\"\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_net(nn.Module):\n",
    "    '''\n",
    "    Estimates the best action which would have maximised q value for a given state\n",
    "    '''\n",
    "    def __init__(self, n_obs: int, n_act: int, dev=\"cpu\") -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_obs, 400),\n",
    "            nn.LeakyReLU(0.1), \n",
    "            nn.Linear(400, 300),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(300, n_act),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.dev = dev\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Critic_net(nn.Module):\n",
    "    '''\n",
    "    Estimates the qvalue given in input space and the action performed on this state\n",
    "    '''\n",
    "    def __init__(self, n_obs: int, n_act: int) -> None:\n",
    "        super().__init__()\n",
    "        self.obs_head = nn.Sequential(\n",
    "            nn.Linear(n_obs, 400),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(400+n_act, 300),\n",
    "            nn.LeakyReLU(0.1), \n",
    "            nn.Linear(300, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        obs_enc = self.obs_head(obs)\n",
    "        out = torch.cat([obs_enc, act], dim=1)\n",
    "        out = self.net(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, n_obs, n_act, dev=DEV, rep_steps=REPEAT_STEPS, batch_size=BATCH_SIZE, optimizer=None, epsilon=MIN_EPSILON) -> None:\n",
    "        ## REPLAY BUFFER\n",
    "        # 1. always keep some old experiences in replay_buf_initial to prevent catastrophic forgetting.\n",
    "        self.buff_maxlen = BUFFER_MAXLEN\n",
    "        self.replay_buf_initial = collections.deque(maxlen = REPLAY_INITIAL)\n",
    "        self.replay_buf= collections.deque(maxlen = self.buff_maxlen - REPLAY_INITIAL)\n",
    "\n",
    "        ## NETWORKS\n",
    "        self.action_net = Agent_net(n_obs, n_act, DEV).to(DEV)\n",
    "        self.action_net_targ = Agent_net(n_obs, n_act).to(DEV).requires_grad_(False)\n",
    "        self.critic_net = Critic_net(n_obs, n_act).to(DEV)\n",
    "        self.critic_net_targ = Critic_net(n_obs, n_act).to(DEV).requires_grad_(False)\n",
    "        self.actor_optim = optim.Adam(self.action_net.parameters(), lr=LR_AGENT)\n",
    "        self.critic_optim = optim.Adam(self.critic_net.parameters(), lr=LR_CRITIC)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "        ## EPISODE and STATE variables\n",
    "        self.ep_score = 0.\n",
    "        self.dev = DEV\n",
    "        self.objective = nn.MSELoss()\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.optim = optimizer\n",
    "        self.rep_steps = REPEAT_STEPS\n",
    "        self.curr_ep_r = 0.\n",
    "        self.epsilon = MAX_EPSILON\n",
    "\n",
    "    def add_transn(self, s, s_next, r, a, is_done):\n",
    "        '''Add new transn tuple in replay buffer'''\n",
    "        if len(self.replay_buf_initial) < self.replay_buf_initial.maxlen:\n",
    "            self.replay_buf_initial.append((s, s_next, r, a, is_done))\n",
    "        else:\n",
    "            self.replay_buf.append((s, s_next, r, a, is_done))\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def move_k_steps(self, s, env, k=None):\n",
    "        s_new = s\n",
    "        s_new = torch.Tensor(s_new).float().to(self.dev)\n",
    "        a = self.action_net(s_new).cpu().numpy()\n",
    "        a += self.epsilon * np.random.normal(size=a.shape)\n",
    "        a = np.clip(a, -1, 1)\n",
    "        r = 0.\n",
    "        curr_ep_r = self.curr_ep_r\n",
    "        if not k:\n",
    "            k = self.rep_steps\n",
    "        for i in range(k):\n",
    "            s_new, _r, is_done, _  = env.step(a)\n",
    "            r += _r\n",
    "            self.curr_ep_r += _r\n",
    "            if is_done:\n",
    "                self.curr_ep_r = 0.\n",
    "                s_new = env.reset()\n",
    "                break\n",
    "        \n",
    "        return s, s_new, r, a, is_done, curr_ep_r\n",
    "\n",
    "\n",
    "    def get_batch(self, batch_size=BATCH_SIZE):\n",
    "        def get_batch_from_buff(buff, batch_size, s, s_next, r, a, is_done):\n",
    "            minibatch = random.sample(buff, batch_size)\n",
    "            for sample in minibatch:\n",
    "                s.append(sample[0])\n",
    "                s_next.append(sample[1])\n",
    "                r.append(sample[2])\n",
    "                a.append(sample[3])\n",
    "                is_done.append(sample[4])\n",
    "                \n",
    "        s, s_next, r, a, is_done = [], [], [], [], [] \n",
    "        if len(self.replay_buf) > batch_size - batch_size//3:\n",
    "            get_batch_from_buff( self.replay_buf_initial, batch_size // 3, s, s_next, r, a, is_done)\n",
    "            get_batch_from_buff( self.replay_buf, batch_size - batch_size // 3, s, s_next, r, a, is_done)\n",
    "        else:\n",
    "            get_batch_from_buff(self.replay_buf_initial, batch_size, s, s_next, r, a, is_done)\n",
    "            \n",
    "        s = torch.FloatTensor(s).to(self.dev)\n",
    "        s_next = torch.FloatTensor(s_next).to(self.dev)\n",
    "        r = torch.FloatTensor(r).to(self.dev)\n",
    "        a = torch.FloatTensor(a).to(self.dev)\n",
    "        is_done = torch.BoolTensor(is_done).to(self.dev)\n",
    "        return s, s_next, r, a, is_done\n",
    "    \n",
    "\n",
    "    def train_iter(self, batch_size=None, rep_steps=None):\n",
    "        if len(self.replay_buf_initial) + len(self.replay_buf) < REPLAY_START_SIZE:\n",
    "            return None, None\n",
    "\n",
    "        if rep_steps is None:\n",
    "            rep_steps = self.rep_steps\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        s, s_next, r, a, is_done = self.get_batch(batch_size)\n",
    "        # print(s.shape, s_next.shape, r.shape, a.shape, is_done.shape)\n",
    "\n",
    "        #CRITIC\n",
    "        self.critic_optim.zero_grad()\n",
    "        # a_next_t = self.action_net_targ(s_next)\n",
    "        a_next = self.action_net(s_next)\n",
    "        q_m = self.critic_net(s, a)\n",
    "        _q_t = self.critic_net_targ(s_next, a_next)\n",
    "        _q_t[is_done] = 0.\n",
    "        q_t = r.unsqueeze(dim=-1) + ((GAMMA**rep_steps) * _q_t)  \n",
    "        loss_critic = self.mse_loss(q_m, q_t.detach())\n",
    "        loss_critic.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        # ACTOR\n",
    "        self.actor_optim.zero_grad() \n",
    "        a_m = self.action_net(s)\n",
    "        loss_actor = -self.critic_net(s, a_m).mean()\n",
    "        loss_actor.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        return loss_actor, loss_critic\n",
    "        \n",
    "    def save_net(self, epoch, path=\".\"):\n",
    "        model_state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"action_model\": self.action_net.state_dict(),\n",
    "            \"action_optim\": self.actor_optim.state_dict(),\n",
    "            \"critic_model\": self.critic_net.state_dict(),\n",
    "            \"critic_optim\": self.critic_optim.state_dict(),\n",
    "        }\n",
    "        path = f\"{path}/model_{epoch}.pt\"\n",
    "        torch.save(model_state, path)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_targ_nets(self, alpha):\n",
    "        def update_targ_net(net, targ_net):\n",
    "            net_dict = net.state_dict()\n",
    "            targ_net_dict = targ_net.state_dict()\n",
    "            for k, v in net_dict.items():\n",
    "                targ_net_dict[k] = targ_net_dict[k] * alpha + v * (1-alpha)\n",
    "            targ_net.load_state_dict(targ_net_dict)\n",
    "\n",
    "        update_targ_net(self.action_net, self.action_net_targ)\n",
    "        update_targ_net(self.critic_net, self.critic_net_targ)\n",
    "        \n",
    "\n",
    "    def load_net(self, fname):\n",
    "        model_state = torch.load(fname, map_location=self.dev)\n",
    "        self.net.load_state_dict(model_state['model'])\n",
    "        self.optimizer.load_state_dict(model_state['optim'])\n",
    "        return model_state[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Sep  1 2022 13:08:43\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir=/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/pybullet_envs/bullet\n",
      "urdf_root=/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/pybullet_data\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "(28,)\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(comment=f\"{ENV_NAME}_ddpg\")\n",
    "env = gym.make(ENV_NAME)\n",
    "print(env.observation_space.shape)\n",
    "print(env.action_space.shape)\n",
    "agent = Agent(\n",
    "    env.observation_space.shape[0],\n",
    "    env.action_space.shape[0],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "0it [00:00, ?it/s]/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "1999it [00:56, 39.14it/s]/tmp/ipykernel_1057376/86196317.py:120: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484803030/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  s = torch.FloatTensor(s).to(self.dev)\n",
      "last ep length : 15, last episode reward : 0.6180, loss agent = -0.0650, loss critic = 0.0000: : 374930it [3:41:36, 29.98it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/anirudh/MISC/playground/RL/DPG.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudh/MISC/playground/RL/DPG.ipynb#ch0000013?line=9'>10</a>\u001b[0m best_r \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudh/MISC/playground/RL/DPG.ipynb#ch0000013?line=10'>11</a>\u001b[0m \u001b[39mwhile\u001b[39;00m(\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudh/MISC/playground/RL/DPG.ipynb#ch0000013?line=11'>12</a>\u001b[0m     \u001b[39m# gather exp\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/anirudh/MISC/playground/RL/DPG.ipynb#ch0000013?line=12'>13</a>\u001b[0m     s, s_next, r, a, is_done, ep_r \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mmove_k_steps(s, env, k\u001b[39m=\u001b[39;49mREPEAT_STEPS)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudh/MISC/playground/RL/DPG.ipynb#ch0000013?line=13'>14</a>\u001b[0m     agent\u001b[39m.\u001b[39madd_transn(s, s_next, r, a, is_done)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudh/MISC/playground/RL/DPG.ipynb#ch0000013?line=14'>15</a>\u001b[0m     s \u001b[39m=\u001b[39m s_next\n",
      "File \u001b[0;32m~/miniconda3/envs/work_env/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/anirudh/MISC/playground/RL/DPG.ipynb Cell 5'\u001b[0m in \u001b[0;36mAgent.move_k_steps\u001b[0;34m(self, s, env, k)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudh/MISC/playground/RL/DPG.ipynb#ch0000004?line=94'>95</a>\u001b[0m     \u001b[39mif\u001b[39;00m is_done:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudh/MISC/playground/RL/DPG.ipynb#ch0000004?line=95'>96</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurr_ep_r \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/anirudh/MISC/playground/RL/DPG.ipynb#ch0000004?line=96'>97</a>\u001b[0m         s_new \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mreset()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anirudh/MISC/playground/RL/DPG.ipynb#ch0000004?line=97'>98</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/anirudh/MISC/playground/RL/DPG.ipynb#ch0000004?line=99'>100</a>\u001b[0m \u001b[39mreturn\u001b[39;00m s, s_new, r, a, is_done, curr_ep_r\n",
      "File \u001b[0;32m~/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/time_limit.py:86\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/time_limit.py?line=76'>77</a>\u001b[0m \u001b[39m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/time_limit.py?line=77'>78</a>\u001b[0m \n\u001b[1;32m     <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/time_limit.py?line=78'>79</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/time_limit.py?line=82'>83</a>\u001b[0m \u001b[39m    The reset environment\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/time_limit.py?line=83'>84</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/time_limit.py?line=84'>85</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/time_limit.py?line=85'>86</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:42\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py?line=39'>40</a>\u001b[0m \u001b[39m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py?line=40'>41</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py?line=41'>42</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/core.py:415\u001b[0m, in \u001b[0;36mWrapper.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/core.py?line=412'>413</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[ObsType, Tuple[ObsType, \u001b[39mdict\u001b[39m]]:\n\u001b[1;32m    <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/core.py?line=413'>414</a>\u001b[0m     \u001b[39m\"\"\"Resets the environment with kwargs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/core.py?line=414'>415</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/env_checker.py:47\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/env_checker.py?line=44'>45</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m env_reset_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/env_checker.py?line=45'>46</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/env_checker.py?line=46'>47</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/work_env/lib/python3.10/site-packages/pybullet_envs/bullet/minitaur_gym_env.py:210\u001b[0m, in \u001b[0;36mMinitaurBulletEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/pybullet_envs/bullet/minitaur_gym_env.py?line=207'>208</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pd_control_enabled \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accurate_motor_model_enabled:\n\u001b[1;32m    <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/pybullet_envs/bullet/minitaur_gym_env.py?line=208'>209</a>\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mminitaur\u001b[39m.\u001b[39mApplyAction([math\u001b[39m.\u001b[39mpi \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m8\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/pybullet_envs/bullet/minitaur_gym_env.py?line=209'>210</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pybullet_client\u001b[39m.\u001b[39;49mstepSimulation()\n\u001b[1;32m    <a href='file:///home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/pybullet_envs/bullet/minitaur_gym_env.py?line=210'>211</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_noisy_observation()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "ep_rewards = collections.deque([0. for _ in range(100)], maxlen=100)\n",
    "r_mean = 0.\n",
    "best_length = None\n",
    "i = 0\n",
    "s = env.reset()\n",
    "pbar = tqdm()\n",
    "last_ep_rew = 0.\n",
    "ep_len = 0\n",
    "last_ep_len = 0\n",
    "best_r = None\n",
    "while(True):\n",
    "    # gather exp\n",
    "    s, s_next, r, a, is_done, ep_r = agent.move_k_steps(s, env, k=REPEAT_STEPS)\n",
    "    agent.add_transn(s, s_next, r, a, is_done)\n",
    "    s = s_next\n",
    "    ep_len += 1\n",
    "    if is_done:\n",
    "        last_ep_rew = ep_r\n",
    "        last_ep_len = ep_len\n",
    "        ep_len = 0\n",
    "        if best_r is None or ep_r > best_r:\n",
    "            best_r = ep_r\n",
    "            agent.save_net(0)\n",
    "    # train\n",
    "    loss_agent, loss_critic = agent.train_iter(batch_size=BATCH_SIZE, rep_steps=REPEAT_STEPS)\n",
    "    agent.update_targ_nets(alpha=1-1e-3)\n",
    "    agent.epsilon = MAX_EPSILON - (MAX_EPSILON - MIN_EPSILON) * (i / 30_000)\n",
    "    agent.epsilon = max(agent.epsilon, MIN_EPSILON)\n",
    "    pbar.update(1)\n",
    "    i += 1\n",
    "    if loss_agent is not None:\n",
    "        if i % 20 == 0:\n",
    "            pbar.set_description(f\"last ep length : {last_ep_len}, last episode reward : {last_ep_rew:.4f}, loss agent = {loss_agent.item():.4f}, loss critic = {loss_critic.item():.4f}\")\n",
    "        writer.add_scalar(\"episode length\", last_ep_len, i)\n",
    "        writer.add_scalar(\"episode reward\", last_ep_rew, i)\n",
    "        writer.add_scalar(\"loss agent\",loss_agent.item(), i)\n",
    "        writer.add_scalar(\"loss critic\", loss_critic.item(), i)\n",
    "        writer.add_scalar(\"epsilon\", agent.epsilon, i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f0cf3828b377fddb9db5616db2584a25cd02953f3f1bafc76c74b10e5fc0cd8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('work_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
