{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
      "  logger.warn(\n",
      "pybullet build time: Sep  1 2022 13:08:43\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2a9b2337d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import pybullet\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = \"MinitaurBulletEnv-v0\"\n",
    "PROC_COUNT = 1\n",
    "NUM_ENVS_PER_PROC = 1\n",
    "BATCH_SIZE = 128\n",
    "LR_AGENT = 1e-4\n",
    "LR_CRITIC = 1e-4\n",
    "BUFFER_MAXLEN = 150_000\n",
    "REPLAY_INITIAL = 30_000\n",
    "REPLAY_START_SIZE = 2_000\n",
    "DEV = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEV)\n",
    "REPEAT_STEPS = 5\n",
    "MAX_EPSILON = 1.0\n",
    "MIN_EPSILON = 1e-3\n",
    "GAMMA = 0.99\n",
    "ENERGY_WEIGHT = 5e-3\n",
    "MODEL_SAVE_PATH = \"models/\"\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getAnglesFromMatrix(rot_mat):\n",
    "    O_x = np.arctan2(rot_mat[3*2+1], rot_mat[3*2+2])\n",
    "    O_y = np.arctan2(-rot_mat[3*2], np.sqrt(rot_mat[3*2+1]**2+rot_mat[3*2+2]))\n",
    "    O_z = np.arctan2(rot_mat[3*1], rot_mat[0])\n",
    "    return (O_x, O_y, O_z)\n",
    "\n",
    "def pos_n_ori(env):\n",
    "    '''\n",
    "    env : Gym env\n",
    "    '''\n",
    "    # current position and orientation\n",
    "    pos = env.minitaur.GetBasePosition()\n",
    "    # pos is 3-tuple [x,y,z]\n",
    "    rot = env.minitaur.GetBaseOrientation()\n",
    "    rot_mat = pybullet.getMatrixFromQuaternion(rot)\n",
    "    rot_ang = getAnglesFromMatrix(rot_mat)\n",
    "    return pos, rot_ang\n",
    "\n",
    "\n",
    "def get_reward(control, pos1, ori1, pos2, ori2, torque, vel):\n",
    "    '''\n",
    "    control : desired movement of the bot\n",
    "        0 : front\n",
    "        1 : right\n",
    "        2 : back\n",
    "        3 : left\n",
    "        4 : turn cw\n",
    "        5 : turn ccw\n",
    "    '''\n",
    "    del_x = pos2[0] - pos1[0]\n",
    "    del_y = pos2[1] - pos1[1]\n",
    "    del_d = np.sqrt(del_x**2 + del_y**2)\n",
    "    # theta = np.arctan2(del_y, del_x)\n",
    "    # del_theta = theta - ori1[2]\n",
    "    del_gamma = ori2[2] - ori1[2]\n",
    "    # del_f = del_d * np.cos(del_theta)\n",
    "    # del_l = del_d * np.sin(del_theta)\n",
    "    # del_b = -del_f\n",
    "    # del_r = -del_l\n",
    "    energy_r = ENERGY_WEIGHT * -np.abs(np.dot(torque, vel))\n",
    "\n",
    "    r = energy_r\n",
    "    if control == 0:\n",
    "        # r = del_f - np.abs(del_l) - np.abs(del_gamma)\n",
    "        # r = del_f\n",
    "        r += del_x\n",
    "    elif control == 1:\n",
    "        # r = del_r - np.abs(del_f) - np.abs(del_gamma)\n",
    "        # r = del_r\n",
    "        r += -del_y\n",
    "    elif control == 2:\n",
    "        # r = del_b - np.abs(del_l) - np.abs(del_gamma)\n",
    "        # r = del_b\n",
    "        r += -del_x\n",
    "    elif control == 3:\n",
    "        # r = del_l - np.abs(del_f) - np.abs(del_gamma)\n",
    "        # r = del_l\n",
    "        r += del_y\n",
    "    elif control == 4:\n",
    "        # r = -del_gamma - np.abs(del_f) - np.abs(del_l)\n",
    "        r += -del_gamma - np.abs(del_d)\n",
    "    elif control == 5:\n",
    "        # r = del_gamma - np.abs(del_f) - np.abs(del_l)\n",
    "        r += del_gamma - np.abs(del_d)\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_net(nn.Module):\n",
    "    '''\n",
    "    Estimates the best action which would have maximised q value for a given state\n",
    "    '''\n",
    "    def __init__(self, n_obs: int, n_act: int, dev=\"cpu\", control_size=6) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_obs+control_size, 800),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(800, 800),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(800, 800),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(800, 800),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(800, n_act),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.dev = dev\n",
    "        self.control_size = control_size\n",
    "\n",
    "    def forward(self, x, control):\n",
    "        '''\n",
    "        control : int in [0, 5] to control bot motion direction\n",
    "        x : observation\n",
    "        '''\n",
    "        one_hot = torch.zeros((x.shape[0], self.control_size), device=self.dev, dtype=torch.float32)\n",
    "        for i in range(len(control)):\n",
    "            one_hot[i, control[i]] = 1\n",
    "        x = torch.cat((x, one_hot), dim=1)\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Critic_net(nn.Module):\n",
    "    '''\n",
    "    Estimates the qvalue given in input space and the action performed on this state\n",
    "    '''\n",
    "    def __init__(self, n_obs: int, n_act: int) -> None:\n",
    "        super().__init__()\n",
    "        self.obs_head = nn.Sequential(\n",
    "            nn.Linear(n_obs, 400),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(400+n_act, 300),\n",
    "            nn.LeakyReLU(0.1), \n",
    "            nn.Linear(300, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        obs_enc = self.obs_head(obs)\n",
    "        out = torch.cat([obs_enc, act], dim=1)\n",
    "        out = self.net(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, n_obs, n_act, dev=DEV, rep_steps=REPEAT_STEPS, batch_size=BATCH_SIZE, optimizer=None, epsilon=MIN_EPSILON) -> None:\n",
    "        ## REPLAY BUFFER\n",
    "        # 1. always keep some old experiences in replay_buf_initial to prevent catastrophic forgetting.\n",
    "        self.buff_maxlen = BUFFER_MAXLEN\n",
    "        self.replay_buf_initial = collections.deque(maxlen = REPLAY_INITIAL)\n",
    "        self.replay_buf= collections.deque(maxlen = self.buff_maxlen - REPLAY_INITIAL)\n",
    "\n",
    "        ## NETWORKS\n",
    "        self.action_net = Agent_net(n_obs, n_act, DEV).to(DEV)\n",
    "        self.action_net_targ = Agent_net(n_obs, n_act).to(DEV).requires_grad_(False)\n",
    "        self.critic_net = Critic_net(n_obs, n_act).to(DEV)\n",
    "        self.critic_net_targ = Critic_net(n_obs, n_act).to(DEV).requires_grad_(False)\n",
    "        self.actor_optim = optim.Adam(self.action_net.parameters(), lr=LR_AGENT)\n",
    "        self.critic_optim = optim.Adam(self.critic_net.parameters(), lr=LR_CRITIC)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "        ## EPISODE and STATE variables\n",
    "        self.ep_score = 0.\n",
    "        self.dev = DEV\n",
    "        self.objective = nn.MSELoss()\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.optim = optimizer\n",
    "        self.rep_steps = REPEAT_STEPS\n",
    "        self.curr_ep_r = 0.\n",
    "        self.epsilon = MAX_EPSILON\n",
    "\n",
    "    def add_transn(self, control, s, s_next, r, a, is_done):\n",
    "        '''Add new transn tuple in replay buffer'''\n",
    "        if len(self.replay_buf_initial) < self.replay_buf_initial.maxlen:\n",
    "            self.replay_buf_initial.append((control, s, s_next, r, a, is_done))\n",
    "        else:\n",
    "            self.replay_buf.append((control, s, s_next, r, a, is_done))\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def move_k_steps(self, s, env, control, ep_len = 1, k=None):\n",
    "        control = [control]\n",
    "        s_new = s\n",
    "        s_new = torch.Tensor(s_new).unsqueeze(0).float().to(self.dev)\n",
    "        a = self.action_net(s_new, control).squeeze().cpu().numpy()\n",
    "        a += self.epsilon * np.random.normal(size=a.shape)\n",
    "        a = np.clip(a, -1, 1)\n",
    "        r = 0.\n",
    "        curr_ep_r = self.curr_ep_r\n",
    "        pos1, rot1 = pos_n_ori(env)\n",
    "        if not k:\n",
    "            k = self.rep_steps\n",
    "        for i in range(k):\n",
    "            s_new, _, is_done, _  = env.step(a)\n",
    "            if is_done: \n",
    "                break\n",
    "        \n",
    "        pos2, rot2 = pos_n_ori(env)\n",
    "        torque = env.minitaur.GetMotorTorques()\n",
    "        vel = env.minitaur.GetMotorVelocities()\n",
    "        r = get_reward(control[0], pos1, rot1, pos2, rot2, torque, vel)\n",
    "        # r *= np.log(2 * ep_len)\n",
    "        curr_ep_r += r\n",
    "        if is_done:\n",
    "            s_new = env.reset()\n",
    "            self.curr_ep_r = 0.\n",
    "        \n",
    "        return s, s_new, r, a, is_done, curr_ep_r\n",
    "\n",
    "\n",
    "    def get_batch(self, batch_size=BATCH_SIZE):\n",
    "        def get_batch_from_buff(buff, batch_size, control, s, s_next, r, a, is_done):\n",
    "            minibatch = random.sample(buff, batch_size)\n",
    "            for sample in minibatch:\n",
    "                control.append(sample[0])\n",
    "                s.append(sample[1])\n",
    "                s_next.append(sample[2])\n",
    "                r.append(sample[3])\n",
    "                a.append(sample[4])\n",
    "                is_done.append(sample[5])\n",
    "                \n",
    "        control, s, s_next, r, a, is_done = [], [], [], [], [], [] \n",
    "        if len(self.replay_buf) > batch_size - batch_size//3:\n",
    "            get_batch_from_buff( self.replay_buf_initial, batch_size // 3, control, s, s_next, r, a, is_done)\n",
    "            get_batch_from_buff( self.replay_buf, batch_size - batch_size // 3, control, s, s_next, r, a, is_done)\n",
    "        else:\n",
    "            get_batch_from_buff(self.replay_buf_initial, batch_size, control, s, s_next, r, a, is_done)\n",
    "            \n",
    "        s = torch.FloatTensor(s).to(self.dev)\n",
    "        s_next = torch.FloatTensor(s_next).to(self.dev)\n",
    "        r = torch.FloatTensor(r).to(self.dev)\n",
    "        a = torch.FloatTensor(a).to(self.dev)\n",
    "        is_done = torch.BoolTensor(is_done).to(self.dev)\n",
    "        return control, s, s_next, r, a, is_done\n",
    "    \n",
    "\n",
    "    def train_iter(self, batch_size=None, rep_steps=None):\n",
    "        if len(self.replay_buf_initial) + len(self.replay_buf) < REPLAY_START_SIZE:\n",
    "            return None, None\n",
    "\n",
    "        if rep_steps is None:\n",
    "            rep_steps = self.rep_steps\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        control, s, s_next, r, a, is_done = self.get_batch(batch_size)\n",
    "        # print(s.shape, s_next.shape, r.shape, a.shape, is_done.shape)\n",
    "\n",
    "        #CRITIC\n",
    "        self.critic_optim.zero_grad()\n",
    "        # a_next_t = self.action_net_targ(s_next)\n",
    "        a_next = self.action_net(s_next, control)\n",
    "        q_m = self.critic_net(s, a)\n",
    "        _q_t = self.critic_net_targ(s_next, a_next)\n",
    "        _q_t[is_done] = 0.\n",
    "        q_t = r.unsqueeze(dim=-1) + ((GAMMA**rep_steps) * _q_t)  \n",
    "        loss_critic = self.mse_loss(q_m, q_t.detach())\n",
    "        loss_critic.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        # ACTOR\n",
    "        self.actor_optim.zero_grad() \n",
    "        a_m = self.action_net(s, control)\n",
    "        loss_actor = -self.critic_net(s, a_m).mean()\n",
    "        loss_actor.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        return loss_actor, loss_critic\n",
    "        \n",
    "    def save_net(self, epoch, path=\".\"):\n",
    "        model_state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"action_model\": self.action_net.state_dict(),\n",
    "            \"action_optim\": self.actor_optim.state_dict(),\n",
    "            \"critic_model\": self.critic_net.state_dict(),\n",
    "            \"critic_optim\": self.critic_optim.state_dict(),\n",
    "        }\n",
    "        path = f\"{path}/model_{epoch}.pt\"\n",
    "        torch.save(model_state, path)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_targ_nets(self, alpha):\n",
    "        def update_targ_net(net, targ_net):\n",
    "            net_dict = net.state_dict()\n",
    "            targ_net_dict = targ_net.state_dict()\n",
    "            for k, v in net_dict.items():\n",
    "                targ_net_dict[k] = targ_net_dict[k] * alpha + v * (1-alpha)\n",
    "            targ_net.load_state_dict(targ_net_dict)\n",
    "\n",
    "        update_targ_net(self.action_net, self.action_net_targ)\n",
    "        update_targ_net(self.critic_net, self.critic_net_targ)\n",
    "        \n",
    "\n",
    "    def load_net(self, fname):\n",
    "        model_state = torch.load(fname, map_location=self.dev)\n",
    "        self.net.load_state_dict(model_state['model'])\n",
    "        self.optimizer.load_state_dict(model_state['optim'])\n",
    "        return model_state[\"epoch\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir=/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/pybullet_envs/bullet\n",
      "urdf_root=/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/pybullet_data\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "(28,)\n",
      "(8,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(comment=f\"{ENV_NAME}_ddpg\")\n",
    "env = gym.make(ENV_NAME)\n",
    "print(env.observation_space.shape)\n",
    "print(env.action_space.shape)\n",
    "agent = Agent(\n",
    "    env.observation_space.shape[0],\n",
    "    env.action_space.shape[0],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "0it [00:00, ?it/s]/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/anirudh/miniconda3/envs/work_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "1998it [01:00, 40.05it/s]/tmp/ipykernel_88085/2328085855.py:142: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484803030/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  s = torch.FloatTensor(s).to(self.dev)\n",
      "last ep length : 2, last episode reward : -0.0005, loss agent = 0.0245, loss critic = 0.0027: : 67795it [1:13:43, 13.77it/s] "
     ]
    }
   ],
   "source": [
    "\n",
    "ep_rewards = collections.deque([0. for _ in range(100)], maxlen=100)\n",
    "r_mean = 0.\n",
    "best_length = None\n",
    "i = 0\n",
    "s = env.reset()\n",
    "pbar = tqdm()\n",
    "last_ep_rew = 0.\n",
    "ep_len = 0\n",
    "last_ep_len = 0\n",
    "best_r = None\n",
    "while(True):\n",
    "    # create new control every 400 frames(400/k iters)\n",
    "    if ep_len%(1000//REPEAT_STEPS) == 0:\n",
    "        control = random.choice([0, 2])\n",
    "    i += 1\n",
    "    ep_len += 1\n",
    "    # gather exp\n",
    "    s, s_next, r, a, is_done, ep_r = agent.move_k_steps(s, env, control, ep_len, k=REPEAT_STEPS)\n",
    "    agent.add_transn(control, s, s_next, r, a, is_done)\n",
    "    s = s_next\n",
    "    if is_done:\n",
    "        last_ep_rew = ep_r\n",
    "        last_ep_len = ep_len\n",
    "        ep_len = 0\n",
    "        if best_r is None or ep_r > best_r:\n",
    "            best_r = ep_r\n",
    "            agent.save_net(0)\n",
    "    # train\n",
    "    loss_agent, loss_critic = agent.train_iter(batch_size=BATCH_SIZE, rep_steps=REPEAT_STEPS)\n",
    "    agent.update_targ_nets(alpha=1-1e-3)\n",
    "    agent.epsilon = MAX_EPSILON - (MAX_EPSILON - MIN_EPSILON) * (i / 35_000)\n",
    "    agent.epsilon = max(agent.epsilon, MIN_EPSILON)\n",
    "    pbar.update(1)\n",
    "    if loss_agent is not None:\n",
    "        if i % 20 == 0:\n",
    "            pbar.set_description(f\"last ep length : {last_ep_len}, last episode reward : {last_ep_rew:.4f}, loss agent = {loss_agent.item():.4f}, loss critic = {loss_critic.item():.4f}\")\n",
    "        writer.add_scalar(\"episode length\", last_ep_len, i)\n",
    "        writer.add_scalar(\"episode reward\", last_ep_rew, i)\n",
    "        writer.add_scalar(\"loss agent\",loss_agent.item(), i)\n",
    "        writer.add_scalar(\"loss critic\", loss_critic.item(), i)\n",
    "        writer.add_scalar(\"epsilon\", agent.epsilon, i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f0cf3828b377fddb9db5616db2584a25cd02953f3f1bafc76c74b10e5fc0cd8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('work_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
